{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartiktongaria/PDFChatbot/blob/main/Chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install tiktoken\n",
        "!pip install sentence-transformers\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "CE29zpitFgCm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eywyk_nw61gm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model names\n",
        "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
        "LLM_FALCON_SMALL = \"tiiuae/falcon-7b-instruct\""
      ],
      "metadata": {
        "id": "uAOrvOxfeBfQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    \"persist_directory\": None,\n",
        "    \"load_in_8bit\": False,\n",
        "    \"embedding\": EMB_SBERT_MPNET_BASE,\n",
        "    \"llm\": LLM_FLAN_T5_BASE,\n",
        "}\n"
      ],
      "metadata": {
        "id": "Sf_V3HpAfCL_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sbert_mpnet():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n"
      ],
      "metadata": {
        "id": "n7QpOnDifGGg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_flan_t5_base(load_in_8bit=False):\n",
        "    model_name = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    return pipeline(\n",
        "        task=\"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=100,\n",
        "        model_kwargs={\"max_length\": 512, \"temperature\": 0.},\n",
        "    )"
      ],
      "metadata": {
        "id": "Q60JVcaBfIR4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_falcon_instruct_small(load_in_8bit=False):\n",
        "    model = \"tiiuae/falcon-7b-instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    hf_pipeline = pipeline(\n",
        "        task=\"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        trust_remote_code=True,\n",
        "        max_new_tokens=100,\n",
        "        model_kwargs={\n",
        "            \"device_map\": \"auto\",\n",
        "            \"load_in_8bit\": load_in_8bit,\n",
        "            \"max_length\": 512,\n",
        "            \"temperature\": 0.01,\n",
        "            \"torch_dtype\": torch.bfloat16,\n",
        "        }\n",
        "    )\n",
        "    return hf_pipeline"
      ],
      "metadata": {
        "id": "0I7DErZIfj21"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pdf using PyPDFLoader\n",
        "\n",
        "pdf_path = \"ENTER_YOU_PDF_FILE _PATH.pdf\" #<<<<<<<<<<-----ENTER YOU PDF FILE PATH---------->>>>>>>>>>#\n",
        "\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Split documents and create text snippets\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=\"cl100k_base\")\n",
        "texts = text_splitter.split_documents(texts)\n",
        "\n",
        "# Embedding\n",
        "persist_directory = config[\"persist_directory\"]\n",
        "embedding = create_sbert_mpnet()\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n",
        "\n",
        "# Language Model\n",
        "load_in_8bit = config[\"load_in_8bit\"]\n",
        "llm = create_flan_t5_base(load_in_8bit=load_in_8bit)\n",
        "hf_llm = HuggingFacePipeline(pipeline=llm)\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
        "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "# Default prompt for flan models\n",
        "if config[\"llm\"] in [LLM_FLAN_T5_BASE]:\n",
        "    question_t5_template = \"\"\"\n",
        "    context: {context}\n",
        "    question: {question}\n",
        "    answer:\n",
        "    \"\"\"\n",
        "    QUESTION_T5_PROMPT = PromptTemplate(\n",
        "        template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT\n",
        "\n",
        "# Query\n",
        "\n",
        "question = \"ENTER_YOUR_QUESTION\" #<<<<<<<<<<<-----ENTER YOUR QUESTION--------------->>>>>>>>>>>#\n",
        "\n",
        "qa.combine_documents_chain.verbose = True\n",
        "qa.return_source_documents = True\n",
        "qa({\"query\": question})"
      ],
      "metadata": {
        "id": "TYheI-v0foVE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95WX6M6iHDK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}